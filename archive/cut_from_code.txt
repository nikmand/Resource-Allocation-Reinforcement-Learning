def eval_loop():
    for i_episode in range(constants.eval_episodes):
        observation = env.reset()  #

        state = quantizator.digitize(observation)
        action = agent.choose_action(state, train=False)

        for step in range(constants.max_steps):
            # env.render()
            observation, reward, done, info = env.step(action)  # takes the specified action
            if done:
                pos = observation[0]
                rot = observation[2]
                if pos < -2.4 or pos > 2.4:
                    print("Terminated due to position")
                print("Episode {} terminated after {} timesteps".format(i_episode, step + 1))
                break

            state = quantizator.digitize(observation)
            action = agent.choose_action(state, train=False)
        else:
            print("Episode {} finished successful!".format(i_episode))


# from memory.py
# Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))

        # if len(self.memory) < self.capacity:
        #     self.memory.append(None)
        # self.memory[self.position] = Transition(*args)
        # self.position = (self.position + 1) % self.capacity

    # def __len__(self):
    #     return len(self.memory)
    #
    # def __getitem__(self, index):
    #     return self.memory[index]


# cut from dqn_agents
# next_state_values = torch.zeros(len(transitions), device=self.device)
# non_final_next_state = torch.tensor([s for s in sample_batch.next_state if s is not None], device=self.device)

# Compute a mask of non-final states
# non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,
#                                         sample_batch.next_state)), dtype=torch.bool, device=self.device)


# cut from test
        # transitions = DataLoader(self.memory, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
        #
        # # issue we don't need to format all the batches but just one, maybe dataloader is not the way to go.
        # for i, x in enumerate(transitions):
        #     output = self.policy_net(x.state)
        #     self.assertEqual([len(x.state), self.env.action_space.n], list(output.shape))
        #     print(output)
        #     result = output.max(1)
        #     print(result)
        #     result = result[1]
        #     print(result)
        #     print(result.view(1, 1))
        #     break


    # Take 100 episode averages and plot them too
    # if len(episode_durations) >= 100:
    #     mean = np.mean(episode_durations[-100:])
    #     means.append(mean)
    #     plt.plot(means)
    # else:
    #     means.append(0)


    # writer.add_text('Parameters', 'Optimizer used: Adam')
    # layout = {'Overview': {'Reward': ['Multiline', ['Reward/Train', 'Reward/Eval']]}}
    # writer.add_custom_scalars(layout)

# not available in spark version that we use
'FMClassifier': ('nikmand/bes:spark-example', 'ml.JavaFMClassifierExample', 'data-svm'),
'FMRegressor': ('nikmand/bes:spark-example', 'ml.JavaFMRegressorExample', 'data-svm'),
'RobustScaler': ('nikmand/bes:spark-example', 'ml.JavaRobustScalerExample', 'data-svm'),

# caused major problems
'pi' : ('nikmand/bes:spark-example', 'SparkPi 500000', None),
'word-count': ('nikmand/bes:spark-example', '--driver-memory 6g --executor-memory 16g JavaWordCount /data/ml-latest/ratings.csv', 'data'),

# problem with dataset
'NaiveBayesExample': ('nikmand/bes:spark-example', 'ml.JavaNaiveBayesExample', 'data-svm')

# working but similar to others
#'DecisionTreeRegression': ('nikmand/bes:spark-example', 'ml.JavaDecisionTreeRegressionExample', 'data-svm'),
#'GradientBoostedTreeClassifier': ('nikmand/bes:spark-example', 'ml.JavaGradientBoostedTreeClassifierExample', 'data-svm'),
#'LogisticRegressionSummary': ('nikmand/bes:spark-example', 'ml.JavaLogisticRegressionSummaryExample', 'data-svm'),
#'RandomForestRegressor': ('nikmand/bes:spark-example', 'ml.JavaRandomForestRegressorExample', 'data-svm'),

# small bes and not ml or analytics
#'StandardScaler': ('nikmand/bes:spark-example', 'ml.JavaStandardScalerExample', 'data-svm'),
#'VectorIndexer': ('nikmand/bes:spark-example', 'ml.JavaVectorIndexerExample', 'data-svm'),

# latency, mpki_be # used to be 2*1e6, 5*1e7, ways_be # 14 me 28 gia mpc # 30 me 120 gia mpki
# 1.5 * 1e7 - 5*1e7 για misses sthn klimaka twn 50ms

